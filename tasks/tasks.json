{
  "tasks": [
    {
      "id": 1,
      "title": "Set up MySQL Database Schema",
      "description": "Design and implement the MySQL database schema for storing Helldivers 2 data including tables for planets, war_status, news, campaigns, major_orders, and planet_history.",
      "details": "Create a MySQL database with the following tables:\n1. `planets` - Store planet data with fields like id, name, sector, region, liberation_status, etc.\n2. `war_status` - Store current war status with timestamp, active players, etc.\n3. `news` - Store news items with id, title, content, publish_date, etc.\n4. `campaigns` - Store campaign data with id, name, description, start_date, end_date, etc.\n5. `major_orders` - Store major orders with id, description, target_planet_id, expiry_time, etc.\n6. `planet_history` - Store historical planet data with planet_id, timestamp, status, and other metrics.\n\nImplement foreign keys between related tables (e.g., planet_history references planets). Add appropriate indexes for efficient querying. Create a schema migration script that can be run to set up the database initially.",
      "testStrategy": "Verify schema creation by running the migration script against a test MySQL instance. Validate that all tables are created with the correct columns, data types, indexes, and constraints. Test inserting sample data into each table to ensure the schema supports all required data.",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Implement API Client for Helldivers 2 API",
      "description": "Create a reusable API client that can fetch data from all required Helldivers 2 API endpoints with proper error handling and retry logic.",
      "details": "Implement a modular API client class that:\n1. Handles HTTP requests to the Helldivers 2 API endpoints\n2. Includes methods for each endpoint: getWarStatus(), getWarInfo(), getNews(), getCampaign(), getPlanetHistory(planetIndex), getMajorOrders(), getPlanets()\n3. Implements error handling with appropriate logging\n4. Includes retry logic with exponential backoff for failed requests\n5. Handles rate limiting if encountered\n6. Returns parsed JSON responses\n\nExample implementation:\n```python\nclass Helldivers2ApiClient:\n    BASE_URL = 'https://helldiverstrainingmanual.com/api/v1/war'\n    \n    def __init__(self, max_retries=3):\n        self.max_retries = max_retries\n        \n    def _make_request(self, endpoint, params=None):\n        url = f\"{self.BASE_URL}/{endpoint}\"\n        retry_count = 0\n        while retry_count < self.max_retries:\n            try:\n                response = requests.get(url, params=params, timeout=10)\n                response.raise_for_status()\n                return response.json()\n            except Exception as e:\n                retry_count += 1\n                if retry_count == self.max_retries:\n                    logging.error(f\"Failed to fetch {url}: {str(e)}\")\n                    raise\n                wait_time = 2 ** retry_count\n                logging.warning(f\"Retrying {url} in {wait_time} seconds...\")\n                time.sleep(wait_time)\n    \n    def get_war_status(self):\n        return self._make_request('status')\n        \n    # Implement other endpoint methods similarly\n```",
      "testStrategy": "Create unit tests for each API endpoint method using mocked HTTP responses. Test error handling by simulating network failures and API errors. Verify retry logic works correctly. Integration test with the actual API to ensure the client can fetch and parse real data correctly.",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Implement Data Transformers",
      "description": "Create data transformer modules that convert API JSON responses to database-ready formats for each data type.",
      "details": "Implement a set of transformer classes or functions that:\n1. Take raw API JSON as input\n2. Validate and clean the data\n3. Transform the data into the format required by the database schema\n4. Handle any data type conversions or normalization needed\n\nCreate separate transformers for each data type:\n- WarStatusTransformer\n- PlanetTransformer\n- NewsTransformer\n- CampaignTransformer\n- MajorOrderTransformer\n- PlanetHistoryTransformer\n\nExample implementation:\n```python\nclass PlanetTransformer:\n    def transform(self, planet_data):\n        \"\"\"Transform raw planet data from API to database format\"\"\"\n        transformed_planets = []\n        for planet in planet_data:\n            transformed_planet = {\n                'planet_id': planet.get('index'),\n                'name': planet.get('name'),\n                'sector': planet.get('sector'),\n                'region': planet.get('region'),\n                'liberation_status': planet.get('liberation', 0),\n                'players': planet.get('players', 0),\n                'max_health': planet.get('maxHealth', 0),\n                'current_health': planet.get('health', 0),\n                'position_x': planet.get('position', {}).get('x', 0),\n                'position_y': planet.get('position', {}).get('y', 0),\n                'position_z': planet.get('position', {}).get('z', 0),\n                'last_updated': datetime.now()\n            }\n            transformed_planets.append(transformed_planet)\n        return transformed_planets\n```\n\nImplement similar transformers for all data types.",
      "testStrategy": "Create unit tests for each transformer using sample API responses. Verify that the transformers correctly handle all expected fields, data types, and edge cases. Test with incomplete or malformed data to ensure proper error handling and validation.",
      "priority": "medium",
      "dependencies": [
        1,
        2
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Implement Database Access Layer",
      "description": "Create a database access layer that handles connections to MySQL and provides methods for inserting, updating, and querying data.",
      "details": "Implement a database access layer that:\n1. Manages MySQL connection using environment variables or config file for credentials\n2. Implements connection pooling for efficient database access\n3. Provides methods for inserting and updating data for each table\n4. Uses transactions to ensure data consistency\n5. Implements upsert logic to avoid duplicates\n\nExample implementation:\n```python\nclass DatabaseManager:\n    def __init__(self, config):\n        self.config = config\n        self.connection_pool = None\n        self._init_connection_pool()\n        \n    def _init_connection_pool(self):\n        self.connection_pool = mysql.connector.pooling.MySQLConnectionPool(\n            pool_name=\"helldivers2_pool\",\n            pool_size=5,\n            host=self.config.get('DB_HOST'),\n            user=self.config.get('DB_USER'),\n            password=self.config.get('DB_PASSWORD'),\n            database=self.config.get('DB_NAME')\n        )\n    \n    def get_connection(self):\n        return self.connection_pool.get_connection()\n    \n    def upsert_planets(self, planets_data):\n        connection = self.get_connection()\n        cursor = connection.cursor()\n        try:\n            for planet in planets_data:\n                query = \"\"\"INSERT INTO planets \n                          (planet_id, name, sector, region, liberation_status, players, max_health, current_health, position_x, position_y, position_z, last_updated) \n                          VALUES (%(planet_id)s, %(name)s, %(sector)s, %(region)s, %(liberation_status)s, %(players)s, %(max_health)s, %(current_health)s, %(position_x)s, %(position_y)s, %(position_z)s, %(last_updated)s)\n                          ON DUPLICATE KEY UPDATE \n                          name=VALUES(name), sector=VALUES(sector), region=VALUES(region), liberation_status=VALUES(liberation_status), \n                          players=VALUES(players), max_health=VALUES(max_health), current_health=VALUES(current_health), \n                          position_x=VALUES(position_x), position_y=VALUES(position_y), position_z=VALUES(position_z), \n                          last_updated=VALUES(last_updated)\"\"\"\n                cursor.execute(query, planet)\n            connection.commit()\n        except Exception as e:\n            connection.rollback()\n            raise e\n        finally:\n            cursor.close()\n            connection.close()\n    \n    # Implement similar methods for other data types\n```\n\nImplement similar methods for all data types.",
      "testStrategy": "Create unit tests using a test database. Test connection management, transaction handling, and error recovery. Verify that upsert logic works correctly by inserting the same data twice. Test with various data scenarios including updates to existing records.",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement War Status Data Fetcher and Storage",
      "description": "Create a module that fetches war status data from the API, transforms it, and stores it in the database.",
      "details": "Implement a WarStatusFetcher class that:\n1. Uses the API client to fetch current war status data\n2. Uses the transformer to convert the data to database format\n3. Uses the database manager to store the data\n4. Handles errors and logs the process\n\nExample implementation:\n```python\nclass WarStatusFetcher:\n    def __init__(self, api_client, transformer, db_manager):\n        self.api_client = api_client\n        self.transformer = transformer\n        self.db_manager = db_manager\n        self.logger = logging.getLogger(__name__)\n        \n    def fetch_and_store(self):\n        try:\n            self.logger.info(\"Fetching war status data\")\n            war_status_data = self.api_client.get_war_status()\n            \n            self.logger.info(\"Transforming war status data\")\n            transformed_data = self.transformer.transform(war_status_data)\n            \n            self.logger.info(\"Storing war status data\")\n            self.db_manager.upsert_war_status(transformed_data)\n            \n            self.logger.info(\"War status data updated successfully\")\n            return True\n        except Exception as e:\n            self.logger.error(f\"Error updating war status data: {str(e)}\")\n            return False\n```",
      "testStrategy": "Create unit tests with mocked dependencies to verify the fetcher correctly orchestrates the API client, transformer, and database manager. Test error handling by simulating failures at each step. Create an integration test that runs the full fetch-transform-store pipeline with test data.",
      "priority": "medium",
      "dependencies": [
        2,
        3,
        4
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Implement Planet Data Fetcher and Storage",
      "description": "Create a module that fetches planet data from the API, transforms it, and stores it in the database.",
      "details": "Implement a PlanetFetcher class that:\n1. Uses the API client to fetch planet data\n2. Uses the transformer to convert the data to database format\n3. Uses the database manager to store the data\n4. Handles errors and logs the process\n\nExample implementation:\n```python\nclass PlanetFetcher:\n    def __init__(self, api_client, transformer, db_manager):\n        self.api_client = api_client\n        self.transformer = transformer\n        self.db_manager = db_manager\n        self.logger = logging.getLogger(__name__)\n        \n    def fetch_and_store(self):\n        try:\n            self.logger.info(\"Fetching planet data\")\n            planet_data = self.api_client.get_planets()\n            \n            self.logger.info(\"Transforming planet data\")\n            transformed_data = self.transformer.transform(planet_data)\n            \n            self.logger.info(\"Storing planet data\")\n            self.db_manager.upsert_planets(transformed_data)\n            \n            self.logger.info(\"Planet data updated successfully\")\n            return True\n        except Exception as e:\n            self.logger.error(f\"Error updating planet data: {str(e)}\")\n            return False\n```",
      "testStrategy": "Create unit tests with mocked dependencies to verify the fetcher correctly orchestrates the API client, transformer, and database manager. Test error handling by simulating failures at each step. Create an integration test that runs the full fetch-transform-store pipeline with test data.",
      "priority": "medium",
      "dependencies": [
        2,
        3,
        4
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Implement Remaining Data Fetchers and Storage",
      "description": "Create modules for fetching, transforming, and storing news, campaign, major orders, and planet history data.",
      "details": "Implement fetcher classes for the remaining data types:\n1. NewsFetcher - fetches and stores news data\n2. CampaignFetcher - fetches and stores campaign data\n3. MajorOrdersFetcher - fetches and stores major orders data\n4. PlanetHistoryFetcher - fetches and stores planet history data\n\nEach fetcher should follow the same pattern as the WarStatusFetcher and PlanetFetcher, with appropriate modifications for the specific data type.\n\nThe PlanetHistoryFetcher will need to iterate through all planets to fetch their history:\n\n```python\nclass PlanetHistoryFetcher:\n    def __init__(self, api_client, transformer, db_manager):\n        self.api_client = api_client\n        self.transformer = transformer\n        self.db_manager = db_manager\n        self.logger = logging.getLogger(__name__)\n        \n    def fetch_and_store(self):\n        try:\n            # First get all planets to know their indices\n            planets = self.api_client.get_planets()\n            success_count = 0\n            failure_count = 0\n            \n            for planet in planets:\n                planet_index = planet.get('index')\n                try:\n                    self.logger.info(f\"Fetching history for planet {planet.get('name')} (index: {planet_index})\")\n                    history_data = self.api_client.get_planet_history(planet_index)\n                    \n                    transformed_data = self.transformer.transform(history_data, planet_index)\n                    self.db_manager.upsert_planet_history(transformed_data)\n                    \n                    success_count += 1\n                except Exception as e:\n                    self.logger.error(f\"Error fetching history for planet {planet_index}: {str(e)}\")\n                    failure_count += 1\n            \n            self.logger.info(f\"Planet history update completed. Success: {success_count}, Failures: {failure_count}\")\n            return success_count > 0\n        except Exception as e:\n            self.logger.error(f\"Error in planet history update process: {str(e)}\")\n            return False\n```\n\nImplement similar fetchers for all remaining data types.",
      "testStrategy": "Create unit tests with mocked dependencies for each fetcher. Test error handling and recovery, especially for the PlanetHistoryFetcher which needs to handle multiple API calls. Create integration tests that verify each fetcher can successfully process real or simulated API data.",
      "priority": "medium",
      "dependencies": [
        2,
        3,
        4
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Implement Main Update Orchestrator",
      "description": "Create a main orchestrator that runs all data fetchers in the correct order and manages the overall update process.",
      "details": "Implement an UpdateOrchestrator class that:\n1. Initializes all required components (API client, transformers, database manager, fetchers)\n2. Runs all fetchers in the appropriate order\n3. Handles errors from individual fetchers without stopping the entire process\n4. Logs the overall update process and results\n5. Provides a summary of the update (success/failure counts)\n\nExample implementation:\n```python\nclass UpdateOrchestrator:\n    def __init__(self, config):\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n        \n        # Initialize components\n        self.api_client = Helldivers2ApiClient(max_retries=3)\n        self.db_manager = DatabaseManager(config)\n        \n        # Initialize transformers\n        self.war_status_transformer = WarStatusTransformer()\n        self.planet_transformer = PlanetTransformer()\n        self.news_transformer = NewsTransformer()\n        self.campaign_transformer = CampaignTransformer()\n        self.major_orders_transformer = MajorOrdersTransformer()\n        self.planet_history_transformer = PlanetHistoryTransformer()\n        \n        # Initialize fetchers\n        self.war_status_fetcher = WarStatusFetcher(self.api_client, self.war_status_transformer, self.db_manager)\n        self.planet_fetcher = PlanetFetcher(self.api_client, self.planet_transformer, self.db_manager)\n        self.news_fetcher = NewsFetcher(self.api_client, self.news_transformer, self.db_manager)\n        self.campaign_fetcher = CampaignFetcher(self.api_client, self.campaign_transformer, self.db_manager)\n        self.major_orders_fetcher = MajorOrdersFetcher(self.api_client, self.major_orders_transformer, self.db_manager)\n        self.planet_history_fetcher = PlanetHistoryFetcher(self.api_client, self.planet_history_transformer, self.db_manager)\n    \n    def run_update(self):\n        start_time = datetime.now()\n        self.logger.info(f\"Starting Helldivers 2 data update at {start_time}\")\n        \n        results = {\n            'war_status': False,\n            'planets': False,\n            'news': False,\n            'campaign': False,\n            'major_orders': False,\n            'planet_history': False\n        }\n        \n        # Run fetchers in order\n        results['war_status'] = self.war_status_fetcher.fetch_and_store()\n        results['planets'] = self.planet_fetcher.fetch_and_store()\n        results['news'] = self.news_fetcher.fetch_and_store()\n        results['campaign'] = self.campaign_fetcher.fetch_and_store()\n        results['major_orders'] = self.major_orders_fetcher.fetch_and_store()\n        results['planet_history'] = self.planet_history_fetcher.fetch_and_store()\n        \n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n        \n        success_count = sum(1 for result in results.values() if result)\n        failure_count = len(results) - success_count\n        \n        self.logger.info(f\"Update completed in {duration} seconds. Successes: {success_count}, Failures: {failure_count}\")\n        for name, result in results.items():\n            status = \"Success\" if result else \"Failed\"\n            self.logger.info(f\"{name}: {status}\")\n        \n        return success_count == len(results)\n```",
      "testStrategy": "Create unit tests with mocked fetchers to verify the orchestrator correctly manages the update process. Test error handling by simulating failures in individual fetchers. Create an integration test that runs a full update with test data or against a test database.",
      "priority": "high",
      "dependencies": [
        5,
        6,
        7
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Implement Configuration and Logging System",
      "description": "Create a configuration system for database credentials and other settings, and implement a comprehensive logging system.",
      "details": "Implement a configuration system that:\n1. Loads settings from environment variables and/or a config file\n2. Provides default values for optional settings\n3. Validates required settings\n\nImplement a logging system that:\n1. Logs to both console and file\n2. Includes timestamps, log levels, and contextual information\n3. Rotates log files to prevent excessive disk usage\n4. Configures different log levels for different components\n\nExample implementation:\n```python\nclass Config:\n    def __init__(self, config_file=None):\n        self.settings = {\n            'DB_HOST': 'localhost',\n            'DB_PORT': 3306,\n            'DB_NAME': 'helldivers2',\n            'DB_USER': None,\n            'DB_PASSWORD': None,\n            'LOG_LEVEL': 'INFO',\n            'LOG_FILE': 'helldivers2_pipeline.log',\n            'UPDATE_INTERVAL_HOURS': 24\n        }\n        \n        # Load from config file if provided\n        if config_file and os.path.exists(config_file):\n            with open(config_file, 'r') as f:\n                file_config = json.load(f)\n                self.settings.update(file_config)\n        \n        # Override with environment variables\n        for key in self.settings.keys():\n            env_value = os.environ.get(key)\n            if env_value is not None:\n                # Convert types as needed\n                if isinstance(self.settings[key], int):\n                    self.settings[key] = int(env_value)\n                else:\n                    self.settings[key] = env_value\n        \n        # Validate required settings\n        required_settings = ['DB_USER', 'DB_PASSWORD', 'DB_NAME']\n        missing_settings = [s for s in required_settings if self.settings[s] is None]\n        if missing_settings:\n            raise ValueError(f\"Missing required settings: {', '.join(missing_settings)}\")\n    \n    def get(self, key, default=None):\n        return self.settings.get(key, default)\n\ndef setup_logging(config):\n    log_level = getattr(logging, config.get('LOG_LEVEL', 'INFO'))\n    log_file = config.get('LOG_FILE')\n    \n    # Create logger\n    logger = logging.getLogger()\n    logger.setLevel(log_level)\n    \n    # Create formatters\n    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    console_formatter = logging.Formatter('%(levelname)s - %(message)s')\n    \n    # File handler with rotation\n    file_handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)\n    file_handler.setFormatter(file_formatter)\n    file_handler.setLevel(log_level)\n    \n    # Console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(console_formatter)\n    console_handler.setLevel(log_level)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger\n```",
      "testStrategy": "Create unit tests for the Config class with various input scenarios (environment variables, config file, missing values). Test the logging setup by capturing log output and verifying format and content. Test log rotation by generating enough logs to trigger rotation.",
      "priority": "medium",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Implement Scheduler and CLI Interface",
      "description": "Create a command-line interface for running updates manually and implement a scheduler for automated daily updates.",
      "details": "Implement a command-line interface that:\n1. Allows running a full update manually\n2. Provides options for updating specific data types\n3. Includes help text and usage examples\n\nImplement a scheduler that:\n1. Uses a cron job or systemd timer to trigger daily updates\n2. Includes a lockfile mechanism to prevent overlapping runs\n3. Handles cleanup after completion or failure\n\nExample implementation:\n```python\nimport argparse\nimport sys\nimport os\nimport fcntl\nimport time\n\ndef create_cli():\n    parser = argparse.ArgumentParser(description='Helldivers 2 Data Pipeline')\n    parser.add_argument('--config', help='Path to config file')\n    parser.add_argument('--update', action='store_true', help='Run a full data update')\n    parser.add_argument('--update-war-status', action='store_true', help='Update war status only')\n    parser.add_argument('--update-planets', action='store_true', help='Update planets only')\n    parser.add_argument('--update-news', action='store_true', help='Update news only')\n    parser.add_argument('--update-campaign', action='store_true', help='Update campaign only')\n    parser.add_argument('--update-major-orders', action='store_true', help='Update major orders only')\n    parser.add_argument('--update-planet-history', action='store_true', help='Update planet history only')\n    parser.add_argument('--daemon', action='store_true', help='Run as a daemon with scheduled updates')\n    parser.add_argument('--interval', type=int, help='Update interval in hours (default: 24)')\n    return parser\n\ndef acquire_lock(lock_file):\n    \"\"\"Acquire an exclusive lock to prevent overlapping runs\"\"\"\n    lock_fd = open(lock_file, 'w')\n    try:\n        fcntl.lockf(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\n        return lock_fd\n    except IOError:\n        return None\n\ndef release_lock(lock_fd):\n    \"\"\"Release the lock\"\"\"\n    if lock_fd:\n        fcntl.lockf(lock_fd, fcntl.LOCK_UN)\n        lock_fd.close()\n\ndef run_daemon(orchestrator, config):\n    \"\"\"Run as a daemon with scheduled updates\"\"\"\n    interval_hours = config.get('UPDATE_INTERVAL_HOURS', 24)\n    interval_seconds = interval_hours * 3600\n    lock_file = '/tmp/helldivers2_pipeline.lock'\n    \n    logger = logging.getLogger(__name__)\n    logger.info(f\"Starting daemon mode with {interval_hours} hour update interval\")\n    \n    while True:\n        lock_fd = acquire_lock(lock_file)\n        if not lock_fd:\n            logger.error(\"Another instance is already running. Exiting.\")\n            sys.exit(1)\n        \n        try:\n            orchestrator.run_update()\n        except Exception as e:\n            logger.error(f\"Error in daemon update: {str(e)}\")\n        finally:\n            release_lock(lock_fd)\n        \n        logger.info(f\"Sleeping for {interval_hours} hours until next update\")\n        time.sleep(interval_seconds)\n\ndef main():\n    parser = create_cli()\n    args = parser.parse_args()\n    \n    # Load config\n    config = Config(args.config)\n    \n    # Setup logging\n    setup_logging(config)\n    logger = logging.getLogger(__name__)\n    \n    # Create orchestrator\n    orchestrator = UpdateOrchestrator(config)\n    \n    if args.daemon:\n        run_daemon(orchestrator, config)\n    elif args.update:\n        orchestrator.run_update()\n    elif args.update_war_status:\n        orchestrator.war_status_fetcher.fetch_and_store()\n    elif args.update_planets:\n        orchestrator.planet_fetcher.fetch_and_store()\n    elif args.update_news:\n        orchestrator.news_fetcher.fetch_and_store()\n    elif args.update_campaign:\n        orchestrator.campaign_fetcher.fetch_and_store()\n    elif args.update_major_orders:\n        orchestrator.major_orders_fetcher.fetch_and_store()\n    elif args.update_planet_history:\n        orchestrator.planet_history_fetcher.fetch_and_store()\n    else:\n        parser.print_help()\n\nif __name__ == '__main__':\n    main()\n```\n\nFor cron setup, create a shell script wrapper and add it to crontab:\n```bash\n#!/bin/bash\n# /usr/local/bin/helldivers2_update.sh\ncd /path/to/helldivers2_pipeline\npython main.py --update --config /path/to/config.json\n```\n\nCrontab entry for daily updates at 2 AM:\n```\n0 2 * * * /usr/local/bin/helldivers2_update.sh >> /var/log/helldivers2_cron.log 2>&1\n```",
      "testStrategy": "Test the CLI by running it with various command-line arguments and verifying the correct methods are called. Test the daemon mode with a short interval in a controlled environment. Test the lock mechanism by attempting to run multiple instances simultaneously.",
      "priority": "medium",
      "dependencies": [
        8,
        9
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 11,
      "title": "Task #11: Configure Test Environment with Dedicated Test Database",
      "description": "Update all test scripts to use a dedicated test database (helldivers2_test) instead of the production database, ensuring test isolation and data safety.",
      "details": "This task involves several key implementation steps:\n\n1. Create a dedicated test database schema (helldivers2_test) that mirrors the production database structure.\n2. Modify all test scripts to use environment variables or configuration settings that point to the test database.\n3. Update the configuration system (from Task #9) to include test-specific database settings.\n4. Implement environment detection to automatically use the test database when in test mode.\n5. Add database connection string overrides in all test files.\n6. Create database setup and teardown scripts that prepare the test database before tests and clean it after tests.\n7. Ensure all CI/CD pipelines are updated to use the test database configuration.\n8. Document the test database setup process in the project documentation.\n9. Add safeguards that prevent test code from connecting to production databases, such as hostname/database name validation.\n10. Update any database fixtures or mock data generators to work with the test database.\n\nThe implementation should leverage the configuration system created in Task #9, extending it to handle different environments (development, testing, production). All database connection strings in test code should be parameterized to use MYSQL_DATABASE environment variable or equivalent configuration setting.",
      "testStrategy": "To verify this task has been completed successfully:\n\n1. Run a comprehensive test suite with database logging enabled to confirm all database connections are using the helldivers2_test database.\n2. Create a test that attempts to connect to the production database and verify it fails with appropriate error messages.\n3. Verify that CI/CD pipelines successfully run tests using the test database.\n4. Perform code review to ensure all test files have been updated to use the test database configuration.\n5. Create a script that analyzes all test files for hardcoded database names and run it to verify no production database references remain.\n6. Test the database setup and teardown scripts to ensure they correctly prepare the test environment.\n7. Verify that running tests does not affect production data by deliberately modifying test data and confirming production remains unchanged.\n8. Test the system with various environment configurations to ensure it correctly selects the appropriate database.\n9. Document test results, including logs showing database connections to the test database.\n10. Perform a pair programming session where another developer reviews the changes and confirms the implementation meets requirements.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 12,
      "title": "Task #12: Implement Test Data Cleanup with Setup and Teardown Logic",
      "description": "Add setup and teardown logic to all test scripts to clean up test data by truncating or deleting from all relevant tables before and after each test execution, ensuring a clean test environment.",
      "details": "This task involves modifying all existing test scripts to include proper setup and teardown procedures:\n\n1. Identify all tables in the helldivers2_test database that are modified during tests.\n2. Create a common utility module (e.g., test_cleanup.py) that contains functions for:\n   - Truncating specific tables\n   - Deleting specific test data based on identifiers\n   - Resetting sequences/auto-increment values if necessary\n3. Implement setup functions that run before each test to ensure a clean starting state:\n   - Should truncate or clean relevant tables\n   - May need to insert baseline/fixture data required by tests\n4. Implement teardown functions that run after each test completes:\n   - Should remove all test data created during the test\n   - Should verify no residual data remains\n5. Modify the test framework to automatically call these setup/teardown functions:\n   - If using pytest, implement fixtures or use setup/teardown hooks\n   - If using unittest, implement setUp() and tearDown() methods\n6. Update documentation to explain the test data cleanup approach\n7. Ensure the cleanup logic works with both individual test runs and full test suite execution\n\nConsider implementing transaction rollbacks where appropriate to improve test performance instead of deleting data after each test. Also ensure that the cleanup logic is robust enough to handle test failures without leaving the database in an inconsistent state.",
      "testStrategy": "To verify this task has been completed successfully:\n\n1. Code Review:\n   - Confirm all test files include appropriate setup/teardown logic\n   - Verify the cleanup utility functions are comprehensive and handle all tables\n   - Check that error handling is in place for cleanup failures\n\n2. Manual Testing:\n   - Run individual tests and inspect the database before and after execution\n   - Verify no test data remains in any tables after test completion\n   - Intentionally cause a test to fail and verify cleanup still occurs\n\n3. Automated Verification:\n   - Create a meta-test that runs after the test suite completes\n   - This meta-test should check all relevant tables for any remaining test data\n   - It should fail if any test artifacts are found in the database\n\n4. Performance Testing:\n   - Measure test execution time before and after implementing cleanup logic\n   - Ensure the cleanup approach doesn't significantly slow down the test suite\n   - If performance issues are found, consider optimizing with bulk operations or transactions\n\n5. Integration Testing:\n   - Verify that tests can be run in any order without interference\n   - Run the full test suite multiple times in succession to ensure consistent results\n   - Confirm tests work correctly when run in parallel (if applicable)",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Task #13: Implement Database Safety Check in Test Scripts",
      "description": "Add a safety mechanism to all test scripts that verifies the database name ends with '_test' before execution, preventing accidental runs against production or staging databases.",
      "details": "The implementation should include:\n\n1. Create a utility function `validateTestDatabase()` that:\n   - Extracts the current database name from the connection configuration\n   - Verifies the database name ends with '_test' using a regex pattern\n   - Throws a clear error and aborts execution if validation fails\n   - Logs a confirmation message if validation passes\n\n2. Modify all test script entry points to:\n   - Call the validation function before any test setup or execution begins\n   - Implement as an early check in the main test runner or in each individual test file\n   - Ensure the check cannot be bypassed or disabled without explicit override\n\n3. For test frameworks with hooks (like Jest, Mocha, pytest):\n   - Implement the check in a beforeAll/beforeEach hook to ensure it runs before any tests\n   - Configure the hook to have the highest priority\n\n4. Add configuration for emergency override:\n   - Create an environment variable (e.g., FORCE_TEST_ON_PROD=true) that can bypass the check\n   - Require multiple confirmations if override is used\n   - Log detailed warnings if override is activated\n\n5. Update documentation:\n   - Document the safety mechanism in the test README\n   - Include examples of expected behavior and error messages\n   - Explain the override process for exceptional situations",
      "testStrategy": "To verify successful implementation:\n\n1. Unit test the validation function:\n   - Test with various database names (e.g., 'app_test', 'test_app', 'app_prod', 'app_staging')\n   - Verify it correctly identifies and rejects non-test databases\n   - Confirm it accepts databases with '_test' suffix\n\n2. Integration testing:\n   - Temporarily modify database connection settings to point to a non-test database\n   - Run test scripts and verify they abort with appropriate error messages\n   - Check that no test operations were performed on the non-test database\n   - Test with the override mechanism and verify it works as expected\n\n3. Code review verification:\n   - Ensure the check is implemented in all test entry points\n   - Verify the check runs before any database operations\n   - Confirm error messages are clear and actionable\n\n4. Documentation check:\n   - Review updated documentation for clarity and completeness\n   - Verify examples match the actual implementation\n\n5. Regression testing:\n   - Run all tests with correct test database configuration\n   - Verify tests still pass and the validation doesn't interfere with normal operation\n   - Measure any performance impact from the added validation",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 14,
      "title": "Task #14: Document Test Database Policy in Project Documentation",
      "description": "Create comprehensive documentation outlining the project's test database policy, specifying that all tests must use a dedicated test database, never run with production credentials, and always clean up test data.",
      "details": "This task involves creating clear documentation of the test database policy that has been implemented in recent tasks (11-13). The documentation should be added to both the README.md and a new CONTRIBUTING.md file if one doesn't exist.\n\nFor the README.md:\n1. Add a section titled \"Testing Policy\" that briefly outlines the key requirements:\n   - All tests must use the dedicated test database (helldivers2_test)\n   - Tests must never run with production credentials\n   - All test data must be cleaned up after test execution\n   - Reference the CONTRIBUTING.md file for more details\n\nFor the CONTRIBUTING.md:\n1. Create this file if it doesn't exist\n2. Add a detailed \"Testing Guidelines\" section that includes:\n   - Explanation of why these policies exist (data safety, test isolation, etc.)\n   - Specific instructions on how to configure test database connections\n   - Code examples showing proper setup and teardown patterns\n   - Instructions for creating test credentials\n   - Explanation of the safety checks implemented in Task #13\n   - Clear warnings about the risks of not following these guidelines\n   - Process for verifying tests are compliant with these policies\n\nThe documentation should be written in a clear, accessible style that both new and existing developers can understand. Include references to the specific implementation details from Tasks #11-13 to ensure consistency between the code and documentation.",
      "testStrategy": "To verify this task has been completed successfully:\n\n1. Review the README.md file:\n   - Confirm it contains a new \"Testing Policy\" section\n   - Verify the section clearly states the three key requirements\n   - Check that it references the CONTRIBUTING.md file for more details\n\n2. Review the CONTRIBUTING.md file:\n   - Confirm it exists and contains a \"Testing Guidelines\" section\n   - Verify it includes detailed explanations of all required policy elements\n   - Check that code examples are provided and accurate\n   - Ensure the documentation references the implementations from Tasks #11-13\n\n3. Conduct a peer review:\n   - Have at least two other team members review the documentation\n   - Verify they understand the policies without additional explanation\n   - Confirm the documentation is comprehensive enough for new developers\n\n4. Test with a new developer:\n   - If possible, have someone unfamiliar with the project read the documentation\n   - Ask them to explain the testing policy back to you\n   - Verify they understand all key requirements\n\n5. Check for consistency:\n   - Ensure the documentation accurately reflects the actual implementations from Tasks #11-13\n   - Verify there are no contradictions between README.md and CONTRIBUTING.md\n\nThe task is complete when all documentation is in place, peer-reviewed, and accurately reflects the implemented testing policies.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Task #15: Implement Database Wipe CLI Command for Fresh Data Pulls",
      "description": "Create a CLI command or script that safely truncates all tables in the production or test database while preserving the schema, enabling fresh data pulls from the Helldivers 2 API.",
      "details": "The implementation should include:\n\n1. A new CLI command or script (e.g., `db-wipe` or similar) that truncates all tables in the specified database.\n2. Safety mechanisms:\n   - Require explicit confirmation with a prompt like \"Are you sure you want to wipe all data from [database_name]? This action cannot be undone. Type 'YES' to confirm.\"\n   - Alternative flag option (e.g., `--force` or `--confirm`) to bypass interactive confirmation for automation purposes.\n   - Database name validation to ensure it matches expected patterns (similar to Task #13's check for test databases).\n3. Schema preservation - only data should be removed, not table structures, constraints, or indexes.\n4. Logging of the operation with timestamp and user information.\n5. Connection handling with proper error management.\n6. Support for both production and test databases with appropriate warnings.\n\nUpdate the project README.md with:\n- Command syntax and examples\n- Warning about data loss\n- Explanation of when/why to use this command\n- Any environment variables or configuration needed\n\nThe implementation should leverage existing database connection utilities in the project and follow the project's coding standards.",
      "testStrategy": "Testing should be thorough to ensure data safety:\n\n1. Unit tests:\n   - Mock database connections to verify confirmation prompts work correctly\n   - Test force flag bypasses confirmation correctly\n   - Verify error handling for connection failures\n\n2. Integration tests (on test database only):\n   - Create a temporary test database with sample data\n   - Run the command against it\n   - Verify all tables are empty but schema remains intact\n   - Check that indexes, constraints, and other schema elements are preserved\n   - Verify logging functionality works as expected\n\n3. Manual testing:\n   - Review the README documentation for clarity and completeness\n   - Test the command on a development environment with sample data\n   - Verify the command works with both interactive confirmation and force flag\n   - Confirm data is completely removed while schema remains intact\n\n4. Safety verification:\n   - Attempt to run against production without proper confirmation and verify it fails\n   - Test database name validation works as expected\n   - Verify the command integrates with existing database safety mechanisms from Task #13\n\nDocument all test results and include examples of command usage in the pull request.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 16,
      "title": "Task #16: Rebuild News API Integration to Match Actual API Response Structure",
      "description": "Update the database schema, import scripts, and related components to correctly handle the actual structure of the news API response, ensuring all fields are properly stored and retrieved.",
      "details": "This task involves a comprehensive update to the news API integration to align with the actual API response structure:\n\n1. Database Schema Updates:\n   - Modify the news table schema to include all fields from the API response (id, published, type, tagIds, message, etc.)\n   - Ensure appropriate data types for each field (e.g., timestamps for published, arrays for tagIds)\n   - Add any necessary indexes for performance optimization\n   - Create migration scripts for schema changes\n\n2. Data Import/Fetch Script Updates:\n   - Refactor the API client code to parse all fields from the news API response\n   - Implement proper error handling for missing or malformed fields\n   - Update the data storage logic to correctly map API response fields to database columns\n   - Ensure proper handling of any special data types (arrays, JSON objects, etc.)\n\n3. Data Wiping Logic:\n   - Create or update truncation/wiping functionality specific to the news table\n   - Ensure this logic is compatible with the CLI wipe command from Task #15\n   - Add safeguards to prevent accidental data loss\n\n4. Documentation:\n   - Document the new database schema with field descriptions\n   - Create a migration guide for future developers\n   - Update API integration documentation to reflect the changes\n   - Document any new functions or methods created\n\n5. Implementation Approach:\n   - Focus on correctness rather than preserving existing data\n   - Implement changes in a feature branch\n   - Follow existing project coding standards and patterns\n   - After completing the news API integration, prepare to repeat the process for other endpoints\n\nNote: This task is a prerequisite for similar updates to other API endpoints, so code should be structured to facilitate reuse where possible.",
      "testStrategy": "The testing strategy should verify all aspects of the updated news API integration:\n\n1. Unit Tests:\n   - Test the API client's ability to correctly parse different response formats\n   - Test mapping functions that convert API responses to database models\n   - Test error handling for edge cases (missing fields, null values, etc.)\n\n2. Integration Tests:\n   - Create tests that mock API responses and verify correct database storage\n   - Test the full fetch-and-store pipeline with sample API responses\n   - Verify that all fields are correctly stored and retrieved from the database\n\n3. Database Tests:\n   - Test schema migrations to ensure they run without errors\n   - Verify that the database schema matches the expected structure\n   - Test data wiping functionality to ensure it properly cleans the news table\n\n4. Validation Tests:\n   - Compare stored data against raw API responses to verify field-by-field accuracy\n   - Test with a variety of real API responses to ensure robustness\n\n5. Test Environment:\n   - All tests must use the test database (following Task #14 policy)\n   - Tests should include the database safety check from Task #13\n   - Tests should clean up after themselves\n\n6. Acceptance Criteria:\n   - All tests pass consistently\n   - Database schema correctly reflects all fields from the API\n   - Import scripts successfully store all data from sample API responses\n   - Documentation is complete and accurate\n   - Code review confirms adherence to project standards",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 17,
      "title": "Task #17: Rebuild War Status API Integration to Match Actual API Response Structure",
      "description": "Update the database schema, import scripts, and related components to correctly handle the actual structure of the war status API response, ensuring all fields are properly stored and retrieved.",
      "details": "This task involves a complete rebuild of the War Status API integration to align with the actual API response structure. The implementation should:\n\n1. Analyze the current War Status API response format to identify all fields and their data types.\n2. Design and implement an updated database schema that accurately reflects this structure.\n3. Create or modify migration scripts following the drop-and-rebuild approach used in Task #16.\n4. Update the data transformer to handle any integer timestamp fields, converting them to MySQL DATETIME strings.\n5. Modify the import scripts to correctly parse and store all fields from the API response.\n6. Update any related components (controllers, models, services) that interact with war status data.\n7. Ensure backward compatibility where possible, or document breaking changes.\n8. Add appropriate error handling for potential API response variations.\n9. Create comprehensive documentation of the new schema and integration process.\n10. Update any existing queries or reports that rely on the war status data.\n\nThe implementation should prioritize data integrity and performance, especially considering that war status data may be frequently accessed and updated.",
      "testStrategy": "Testing for this task should be comprehensive and include:\n\n1. Unit tests:\n   - Test the data transformer to verify correct handling of integer timestamps to DATETIME conversions\n   - Test the import scripts with mock API responses to ensure proper parsing\n   - Test database models for correct field validation and relationships\n\n2. Integration tests:\n   - Test the full import process with sample API responses\n   - Verify that all fields are correctly stored in the database\n   - Test the retrieval of war status data through relevant endpoints\n\n3. Regression tests:\n   - Ensure existing functionality that depends on war status data continues to work\n   - Verify that any reports or dashboards display correct information\n\n4. Performance tests:\n   - Measure import time for typical API responses\n   - Test query performance for common war status data access patterns\n\n5. Manual verification:\n   - Compare the stored data with the raw API response to confirm accuracy\n   - Verify the data is displayed correctly in the application UI\n\nDocument all test cases and results, including before/after comparisons of database structure and data representation. Include screenshots of the working integration in the documentation.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 18,
      "title": "Task #18: Rebuild Planets API Integration to Match Actual API Response Structure",
      "description": "Update the database schema, import scripts, and related components to correctly handle the actual structure of the planets API response, ensuring all fields are properly stored and retrieved.",
      "details": "This task involves a comprehensive rebuild of the Planets API integration to align with the actual API response structure. The implementation should follow these steps:\n\n1. Analyze the current Planets API response structure and identify discrepancies with our existing database schema.\n2. Design an updated database schema that accurately reflects all fields in the API response.\n3. Create a migration script using the drop-and-rebuild approach (as in Task #16) to safely update the schema without data loss.\n4. Update the data transformer to properly handle all fields, paying special attention to:\n   - Integer timestamp fields that need conversion to MySQL DATETIME strings\n   - Nested objects or arrays that may require serialization\n   - Field type mismatches between the API and database\n5. Modify the import scripts to correctly parse and store all fields from the API response.\n6. Update any related components that interact with planet data (controllers, services, etc.).\n7. Revise the API documentation to reflect the changes in the data structure.\n8. Ensure proper error handling for cases where the API response structure might change in the future.\n\nThe implementation should prioritize data integrity while maintaining backward compatibility with existing application features that rely on planet data.",
      "testStrategy": "Testing for this task should be comprehensive and include:\n\n1. Unit tests:\n   - Test the data transformer to ensure it correctly converts API response objects to database entities\n   - Verify timestamp conversion logic works correctly for all date/time fields\n   - Test edge cases like missing fields or null values in the API response\n\n2. Integration tests:\n   - Create mock API responses that match the actual structure\n   - Verify the entire import pipeline correctly processes and stores the data\n   - Test the database schema by inserting sample data and checking constraints\n\n3. Manual verification:\n   - Perform a complete data import from the live Planets API\n   - Verify all fields are correctly stored in the database\n   - Compare a sample of raw API responses with the stored database records\n   - Check that all application features using planet data continue to function correctly\n\n4. Regression testing:\n   - Ensure that existing queries and reports using planet data still work\n   - Verify that any dependent services can still access the required planet information\n\n5. Documentation validation:\n   - Review updated API documentation for accuracy\n   - Confirm schema diagrams match the implemented database structure\n\nThe testing should be documented with screenshots of before/after database records and API responses to demonstrate successful implementation.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 19,
      "title": "Task #19: Rebuild Campaigns API Integration to Match Actual API Response Structure",
      "description": "Update the database schema, import scripts, and related components to correctly handle the actual structure of the campaigns API response, ensuring all fields are properly stored and retrieved.",
      "details": "This task involves a comprehensive rebuild of the Campaigns API integration to align with the actual API response structure. The implementation should follow these steps:\n\n1. Analyze the actual Campaigns API response structure to identify all fields, data types, and relationships.\n2. Design an updated database schema that accurately represents this structure, ensuring proper field types and relationships.\n3. Implement a drop-and-rebuild migration approach (similar to Task #16) to update the database schema without data loss concerns.\n4. Update the import scripts to correctly parse and transform the API response data.\n5. Pay special attention to integer timestamp fields, which should be converted to MySQL DATETIME strings in the transformer.\n6. Update any related components that interact with campaign data, including services, controllers, and repositories.\n7. Ensure proper error handling for API response variations or missing fields.\n8. Update the API documentation to reflect the new structure and field mappings.\n9. Implement proper logging for debugging and monitoring purposes.\n10. Update any existing unit tests to accommodate the new structure.\n11. Consider performance implications, especially if the campaigns data is frequently accessed or large in volume.",
      "testStrategy": "To verify successful completion of this task, implement the following testing strategy:\n\n1. Unit Tests:\n   - Create unit tests for the transformer to verify correct conversion of integer timestamps to MySQL DATETIME strings.\n   - Test the import scripts with sample API responses to ensure proper parsing and storage.\n   - Verify error handling for edge cases (missing fields, malformed responses).\n\n2. Integration Tests:\n   - Test the complete flow from API response to database storage.\n   - Verify that all fields from the API response are correctly stored in the database.\n   - Test retrieval operations to ensure data integrity is maintained.\n\n3. Migration Testing:\n   - Test the migration script on a staging environment with production-like data.\n   - Verify that the drop-and-rebuild approach works correctly without unexpected side effects.\n\n4. Manual Testing:\n   - Manually inspect the database schema to confirm it matches the API response structure.\n   - Perform a test import with real API data and verify all fields are correctly stored.\n   - Check the application's UI/features that display campaign data to ensure they work correctly with the new structure.\n\n5. Documentation Verification:\n   - Review updated documentation for accuracy and completeness.\n   - Ensure API documentation reflects the actual implementation.\n\n6. Regression Testing:\n   - Verify that other system components that depend on campaign data continue to function correctly.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 20,
      "title": "Task #20: Rebuild Major Orders API Integration to Match Actual API Response Structure",
      "description": "Update the database schema, import scripts, and related components to correctly handle the actual structure of the major orders API response, ensuring all fields are properly stored and retrieved.",
      "details": "This task involves a comprehensive rebuild of the Major Orders API integration to align with the actual API response structure. The implementation should follow these steps:\n\n1. Analyze the actual Major Orders API response structure to identify all fields, data types, and relationships.\n2. Update the database schema using the drop-and-rebuild migration approach (as in Task #16) to accommodate the actual structure.\n3. Modify the import scripts to correctly parse and process the API response data.\n4. Implement data transformations where necessary, particularly checking for integer timestamp fields and converting them to MySQL DATETIME strings in the transformer.\n5. Update any related components that interact with Major Orders data, including:\n   - Data access layers\n   - Service classes\n   - Controllers\n   - View models\n6. Ensure proper error handling for potential API response variations.\n7. Update the API documentation to reflect the changes.\n8. Maintain backward compatibility where possible, or document breaking changes.\n9. Implement appropriate logging for debugging and monitoring purposes.\n10. Update any caching mechanisms if applicable.\n\nThe implementation should prioritize data integrity and performance while ensuring all fields from the API response are correctly mapped to the database schema.",
      "testStrategy": "To verify the successful completion of this task, the following testing approach should be implemented:\n\n1. Unit Tests:\n   - Create unit tests for the data transformer to verify correct handling of integer timestamps to MySQL DATETIME conversions.\n   - Test the import scripts with mock API responses to ensure proper parsing.\n   - Verify error handling for edge cases (missing fields, null values, etc.).\n\n2. Integration Tests:\n   - Test the complete flow from API response to database storage using sample responses.\n   - Verify that all fields are correctly stored and can be retrieved with proper types.\n   - Test the migration process to ensure it correctly rebuilds the schema without data loss.\n\n3. Manual Testing:\n   - Compare the actual API response structure with the database schema to ensure alignment.\n   - Verify that the application correctly displays all Major Orders data.\n   - Test with real API endpoints to ensure compatibility.\n\n4. Regression Testing:\n   - Ensure that existing functionality that depends on Major Orders data continues to work.\n   - Verify that reports or dashboards using this data display correctly.\n\n5. Documentation Verification:\n   - Review updated documentation for accuracy and completeness.\n   - Ensure API documentation reflects the actual structure and field mappings.\n\n6. Performance Testing:\n   - Measure import times for large datasets to ensure acceptable performance.\n   - Verify that database queries remain efficient after schema changes.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Analyze Major Orders API Response Structure",
          "description": "Document the actual API response structure including all fields, data types, and relationships to guide the rebuild process.",
          "dependencies": [],
          "details": "Make API calls to the Major Orders endpoint and capture sample responses. Create a comprehensive mapping document that identifies all fields, their data types, nesting structure, and relationships. Pay special attention to timestamp fields that may need conversion. Compare the actual structure with the current implementation to identify gaps and discrepancies. Document any inconsistencies or edge cases in the API response that will need special handling.\n<info added on 2025-05-14T23:31:54.057Z>\nMake API calls to the Major Orders endpoint and capture sample responses. Create a comprehensive mapping document that identifies all fields, their data types, nesting structure, and relationships. Pay special attention to timestamp fields that may need conversion. Compare the actual structure with the current implementation to identify gaps and discrepancies. Document any inconsistencies or edge cases in the API response that will need special handling.\n\nThe Major Orders API integration is located in the following components:\n- API call: Helldivers2ApiClient.get_major_orders() (src/helldivers_api_client.py)\n- Fetch/store logic: MajorOrdersFetcher (src/major_orders_fetcher.py)\n- Data transformation: MajorOrderTransformer (src/transformers/major_order_transformer.py)\n- Database upsert: DatabaseManager.upsert_major_order (src/database_manager.py)\n\nSample API response structure:\n```\n[\n  {\n    'expiresIn': 466580,\n    'id32': 2814891206,\n    'progress': [1, 1, 1],\n    'setting': {\n      'flags': 1,\n      'overrideBrief': 'Slow the Illuminate advance while inflicting as much damage to the Fleet as possible.',\n      'overrideTitle': 'MAJOR ORDER',\n      'reward': {'amount': 50, 'id32': 897894480, 'type': 1},\n      'rewards': [{'amount': 50, 'id32': 897894480, 'type': 1}],\n      'taskDescription': '',\n      'tasks': [\n        {'type': 13, 'valueTypes': [3, 11, 12], 'values': [1, 1, 3]},\n        {'type': 13, 'valueTypes': [3, 11, 12], 'values': [1, 1, 5]},\n        {'type': 13, 'valueTypes': [3, 11, 12], 'values': [1, 1, 4]}\n      ],\n      'type': 4\n    }\n  }\n]\n```\n\nField mapping and data types:\n- Top-level fields:\n  - expiresIn (integer): Time remaining in seconds\n  - id32 (integer): Unique identifier for the order\n  - progress (array of integers): Completion status for each task\n  - setting (object): Contains detailed configuration of the order\n\n- setting object:\n  - flags (integer): Configuration flags\n  - overrideBrief (string): Description text of the order\n  - overrideTitle (string): Title of the order\n  - reward (object): Primary reward information\n  - rewards (array of objects): List of all rewards\n  - taskDescription (string): Additional task description\n  - tasks (array of objects): List of specific tasks to complete\n  - type (integer): Type identifier for the order\n\n- reward/rewards objects:\n  - amount (integer): Quantity of the reward\n  - id32 (integer): Identifier for the reward type\n  - type (integer): Category of reward\n\n- tasks objects:\n  - type (integer): Task type identifier\n  - valueTypes (array of integers): Identifiers for value types\n  - values (array of integers): Specific values for the task\n\nNext steps:\n1. Compare this structure to the current database schema to identify mismatches\n2. Document field transformations needed (e.g., expiresIn to expiry_time)\n3. Determine how to handle nested objects and arrays in the database\n4. Identify any missing fields in the current implementation\n5. Document the meaning of enumerated values (type, flags, etc.)\n</info added on 2025-05-14T23:31:54.057Z>\n<info added on 2025-05-14T23:33:01.407Z>\nMake API calls to the Major Orders endpoint and capture sample responses. Create a comprehensive mapping document that identifies all fields, their data types, nesting structure, and relationships. Pay special attention to timestamp fields that may need conversion. Compare the actual structure with the current implementation to identify gaps and discrepancies. Document any inconsistencies or edge cases in the API response that will need special handling.\n\n<info added on 2025-05-14T23:31:54.057Z>\nMake API calls to the Major Orders endpoint and capture sample responses. Create a comprehensive mapping document that identifies all fields, their data types, nesting structure, and relationships. Pay special attention to timestamp fields that may need conversion. Compare the actual structure with the current implementation to identify gaps and discrepancies. Document any inconsistencies or edge cases in the API response that will need special handling.\n\nThe Major Orders API integration is located in the following components:\n- API call: Helldivers2ApiClient.get_major_orders() (src/helldivers_api_client.py)\n- Fetch/store logic: MajorOrdersFetcher (src/major_orders_fetcher.py)\n- Data transformation: MajorOrderTransformer (src/transformers/major_order_transformer.py)\n- Database upsert: DatabaseManager.upsert_major_order (src/database_manager.py)\n\nSample API response structure:\n```\n[\n  {\n    'expiresIn': 466580,\n    'id32': 2814891206,\n    'progress': [1, 1, 1],\n    'setting': {\n      'flags': 1,\n      'overrideBrief': 'Slow the Illuminate advance while inflicting as much damage to the Fleet as possible.',\n      'overrideTitle': 'MAJOR ORDER',\n      'reward': {'amount': 50, 'id32': 897894480, 'type': 1},\n      'rewards': [{'amount': 50, 'id32': 897894480, 'type': 1}],\n      'taskDescription': '',\n      'tasks': [\n        {'type': 13, 'valueTypes': [3, 11, 12], 'values': [1, 1, 3]},\n        {'type': 13, 'valueTypes': [3, 11, 12], 'values': [1, 1, 5]},\n        {'type': 13, 'valueTypes': [3, 11, 12], 'values': [1, 1, 4]}\n      ],\n      'type': 4\n    }\n  }\n]\n```\n\nField mapping and data types:\n- Top-level fields:\n  - expiresIn (integer): Time remaining in seconds\n  - id32 (integer): Unique identifier for the order\n  - progress (array of integers): Completion status for each task\n  - setting (object): Contains detailed configuration of the order\n\n- setting object:\n  - flags (integer): Configuration flags\n  - overrideBrief (string): Description text of the order\n  - overrideTitle (string): Title of the order\n  - reward (object): Primary reward information\n  - rewards (array of objects): List of all rewards\n  - taskDescription (string): Additional task description\n  - tasks (array of objects): List of specific tasks to complete\n  - type (integer): Type identifier for the order\n\n- reward/rewards objects:\n  - amount (integer): Quantity of the reward\n  - id32 (integer): Identifier for the reward type\n  - type (integer): Category of reward\n\n- tasks objects:\n  - type (integer): Task type identifier\n  - valueTypes (array of integers): Identifiers for value types\n  - values (array of integers): Specific values for the task\n\nNext steps:\n1. Compare this structure to the current database schema to identify mismatches\n2. Document field transformations needed (e.g., expiresIn to expiry_time)\n3. Determine how to handle nested objects and arrays in the database\n4. Identify any missing fields in the current implementation\n5. Document the meaning of enumerated values (type, flags, etc.)\n</info added on 2025-05-14T23:31:54.057Z>\n\n<info added on 2025-05-15T10:45:22.000Z>\n## API Response vs Current Implementation Gap Analysis\n\n### Current Implementation Limitations\n1. **Transformer Limitations (src/transformers/major_order_transformer.py)**:\n   - Only maps 4 fields: description, target_planet_id, expiry_time, created_at\n   - No handling for nested objects or arrays\n   - No mapping for the API's unique identifier (id32)\n\n2. **Database Schema Limitations (migrations/004_rebuild_planets_and_lookups.sql)**:\n   - Limited to 5 columns: id, description, target_planet_id, expiry_time, created_at\n   - No storage for progress tracking, tasks, rewards, or other metadata\n   - Uses auto-increment ID instead of the API's id32 as primary identifier\n\n3. **Missing Field Mappings**:\n   - expiresIn: Not properly converted to expiry_time (seconds vs. DATETIME)\n   - id32: API's unique identifier not preserved\n   - progress: Task completion status array not stored\n   - setting.flags: Configuration flags not stored\n   - setting.overrideBrief: Likely the source for \"description\" but not explicitly mapped\n   - setting.overrideTitle: Title information lost\n   - setting.reward/rewards: Reward information not stored\n   - setting.taskDescription: Additional task details lost\n   - setting.tasks: Task definitions and requirements not stored\n   - setting.type: Order type classification not stored\n\n### Data Transformation Issues\n- Time representation: expiresIn (seconds) needs conversion to absolute datetime\n- Nested objects require flattening or JSON serialization\n- Arrays (progress, tasks, rewards) need proper handling\n- Enumerated values (type, flags) need interpretation and documentation\n\n### Schema Redesign Considerations\n- Need to preserve API's unique identifier (id32)\n- Consider JSON columns for complex nested structures\n- Possible related tables for tasks and rewards\n- Add missing metadata fields (title, type, flags)\n- Ensure proper indexing for query performance\n\nThis analysis confirms the need for a complete rebuild of the Major Orders integration to properly capture and utilize the full API response structure. The next subtask (20.2: Update Database Schema for Major Orders) should address these gaps with a comprehensive schema redesign.\n</info added on 2025-05-15T10:45:22.000Z>\n</info added on 2025-05-14T23:33:01.407Z>",
          "status": "done",
          "testStrategy": "Verify the completeness of the analysis by comparing documented fields against multiple API responses from different scenarios."
        },
        {
          "id": 2,
          "title": "Update Database Schema for Major Orders",
          "description": "Redesign and implement the database schema to properly store all fields from the actual API response.",
          "dependencies": [
            1
          ],
          "details": "Create a drop-and-rebuild migration (similar to Task #16) that aligns the database schema with the actual API response structure. Define appropriate column types, indexes, and constraints. Ensure proper handling of timestamp fields, converting integer timestamps to MySQL DATETIME format where needed. Consider performance implications for queries and implement appropriate indexing strategies. Document the schema changes thoroughly for future reference.\n<info added on 2025-05-14T23:39:50.640Z>\nCreate a drop-and-rebuild migration (similar to Task #16) that aligns the database schema with the actual API response structure. Define appropriate column types, indexes, and constraints. Ensure proper handling of timestamp fields, converting integer timestamps to MySQL DATETIME format where needed. Consider performance implications for queries and implement appropriate indexing strategies. Document the schema changes thoroughly for future reference.\n\nThe approved schema for the major_orders table will include:\n\n- id32 (BIGINT PRIMARY KEY): API's unique identifier\n- expires_in (INT): Raw seconds from API\n- expiry_time (DATETIME): Calculated from expiresIn + fetch time\n- progress (JSON): Array of progress values\n- flags (INT): From setting.flags\n- override_brief (TEXT): From setting.overrideBrief\n- override_title (VARCHAR(255)): From setting.overrideTitle\n- reward (JSON): Main reward object\n- rewards (JSON): Array of reward objects\n- task_description (TEXT): From setting.taskDescription\n- tasks (JSON): Array of task objects\n- order_type (INT): From setting.type\n- created_at (TIMESTAMP): Record creation time\n- updated_at (TIMESTAMP): Last update time\n\nThe schema will include indexes on expiry_time and order_type columns to optimize query performance. All nested and complex data structures from the API will be stored as JSON fields for maximum flexibility and to accommodate potential future changes to the API structure.\n\nThe migration will completely drop and recreate the table, which is acceptable for this implementation. After completing the schema design, the next steps will be to draft the migration SQL and update the data transformer to properly map API response fields to this new database structure.\n</info added on 2025-05-14T23:39:50.640Z>",
          "status": "done",
          "testStrategy": "Test the migration in a development environment to ensure it runs without errors and verify that the resulting schema can store all fields identified in subtask 1."
        },
        {
          "id": 3,
          "title": "Modify Import Scripts and Data Transformers",
          "description": "Update the import scripts to correctly parse, transform, and store the API response data in the new database schema.",
          "dependencies": [
            1,
            2
          ],
          "details": "Refactor the import scripts to handle the actual API response structure. Implement data transformations for all fields, with special attention to timestamp conversions from integer to DATETIME format. Add validation logic to ensure data integrity during import. Implement error handling for missing or malformed fields in the API response. Create unit tests for the transformation logic to verify correct handling of various data scenarios. Optimize the import process for performance, especially for large data sets.\n<info added on 2025-05-14T23:50:04.287Z>\nThe import pipeline for Major Orders consists of three main components:\n\n1. MajorOrdersFetcher (src/major_orders_fetcher.py) - Responsible for fetching data from the API\n2. MajorOrderTransformer - Handles data transformation to match the database schema\n3. DatabaseManager - Manages database operations including upserts\n\nCurrent Status:\n- The transformer and database upsert logic have already been updated to match the new schema\n- Unit and integration tests confirm these components work correctly with the new schema\n- The fetcher script is successfully integrated with the updated transformer and DB logic\n- End-to-end pipeline tests are passing with the new schema\n\nRemaining Tasks:\n1. Review and enhance error handling in the fetcher and transformer:\n   - Add specific exception handling for API timeouts, connection errors, and malformed responses\n   - Implement logging for all error cases with appropriate severity levels\n   - Create fallback mechanisms for partial data processing when possible\n\n2. Strengthen validation logic in the transformer:\n   - Add comprehensive field validation for all required fields\n   - Implement type checking and conversion for fields that may have inconsistent types\n   - Add boundary validation for numeric fields and format validation for strings\n\n3. Improve robustness against edge cases:\n   - Handle null values, empty arrays, and unexpected data types gracefully\n   - Add defensive coding patterns to prevent processing failures\n   - Implement circuit breakers to prevent cascading failures\n\n4. Optimize performance:\n   - Evaluate and implement batch processing for large datasets\n   - Consider connection pooling for database operations\n   - Profile the import process to identify and address bottlenecks\n\n5. Testing:\n   - Ensure all unit tests cover the new validation and error handling logic\n   - Add integration tests for edge cases and error scenarios\n   - Verify performance under load with realistic data volumes\n</info added on 2025-05-14T23:50:04.287Z>",
          "status": "done",
          "testStrategy": "Create unit tests with mock API responses to verify correct transformation and storage of data. Test with edge cases like missing fields, null values, and unexpected data types."
        },
        {
          "id": 4,
          "title": "Update Related Components and Services",
          "description": "Modify all components that interact with Major Orders data to work with the new schema and data structure.",
          "dependencies": [
            2,
            3
          ],
          "details": "Update data access layers, service classes, controllers, and view models to align with the new database schema. Ensure all CRUD operations work correctly with the updated structure. Implement backward compatibility where possible, or document breaking changes. Update any caching mechanisms to work with the new data structure. Implement comprehensive logging for debugging and monitoring. Review and update any related business logic that depends on Major Orders data.",
          "status": "done",
          "testStrategy": "Create integration tests that verify the full data flow from API import to frontend display. Test all CRUD operations against the new schema."
        },
        {
          "id": 5,
          "title": "Update Documentation and Finalize Implementation",
          "description": "Update API documentation, create migration guides, and perform final testing of the rebuilt integration.",
          "dependencies": [
            3,
            4
          ],
          "details": "Update the API documentation to reflect the changes in data structure and integration. Create migration guides for any breaking changes. Perform comprehensive testing of the entire integration flow. Implement monitoring for the import process to detect any issues in production. Create a rollback plan in case of unexpected issues. Prepare a deployment strategy that minimizes downtime and data loss risk. Document any performance improvements or potential issues for future reference.",
          "status": "done",
          "testStrategy": "Conduct end-to-end testing in a staging environment that mirrors production. Verify that all components work together correctly and that data integrity is maintained throughout the process."
        }
      ]
    },
    {
      "id": 21,
      "title": "Task #21: Rebuild Planet History API Integration to Match Actual API Response Structure",
      "description": "Update the database schema, import scripts, and related components to correctly handle the actual structure of the planet history API response, ensuring all fields are properly stored and retrieved.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "details": "This task involves a comprehensive rebuild of the Planet History API integration to align with the actual API response structure. Key implementation steps include:\n\n1. Analyze the actual Planet History API response structure to identify all fields, data types, and relationships.\n   - Analysis complete: API response includes created_at (ISO timestamp), current_health (int), max_health (int), planet_index (int), player_count (int)\n\n2. Update the database schema to accommodate the actual response structure:\n   - Create a migration script (007_update_planet_history.sql) using ALTER TABLE statements to add the following columns:\n     * current_health (BIGINT)\n     * max_health (BIGINT)\n     * player_count (INT)\n   - Ensure proper field types and constraints are defined\n   - Add appropriate indexes for performance optimization\n   - Migration script has been prepared and is ready for execution on both live and test databases\n\n3. Update the data import scripts to:\n   - Parse all fields from the API response correctly\n   - Implement a transformer to handle data type conversions\n   - Check for integer timestamp fields and convert them to MySQL DATETIME strings\n   - Handle any nested data structures appropriately\n\n4. Update related components:\n   - Modify service layer methods that interact with Planet History data\n   - Update repository/data access layer to reflect new schema\n   - Adjust any controllers or endpoints that expose Planet History data\n\n5. Update documentation:\n   - Update API documentation to reflect new data structure\n   - Document the database schema changes\n   - Update any developer guides related to Planet History data\n\n6. Ensure backward compatibility where possible, or document breaking changes clearly.\n\n7. Follow the established patterns from previous API integration rebuilds (Tasks 18-20) for consistency across the codebase.",
      "testStrategy": "The testing strategy should verify that the Planet History API integration correctly handles the actual API response structure:\n\n1. Unit Tests:\n   - Test the transformer functions to ensure proper conversion of integer timestamps to MySQL DATETIME strings\n   - Test data mapping functions to verify all fields are correctly mapped (created_at, current_health, max_health, planet_index, player_count)\n   - Test repository methods for CRUD operations on the updated schema\n\n2. Integration Tests:\n   - Create tests with mock API responses that match the actual structure\n   - Verify the entire import process works end-to-end\n   - Test that all fields are correctly stored in the database\n   - Verify that retrieved data maintains integrity and matches the original API data\n\n3. Database Tests:\n   - Verify the migration script (007_update_planet_history.sql) executes without errors on both live and test databases\n   - Confirm the schema matches the expected structure after migration\n   - Test queries against the new schema for performance\n   - Verify that the new columns (current_health, max_health, player_count) are properly added and can store the expected data types\n\n4. API Tests:\n   - Test any endpoints that expose Planet History data\n   - Verify the response format matches documentation\n   - Test edge cases like missing fields or null values\n\n5. Manual Testing:\n   - Perform a complete import of real Planet History data\n   - Verify data integrity through admin interface or reporting tools\n   - Check for any performance issues with large datasets\n\n6. Regression Testing:\n   - Ensure other system components that depend on Planet History data continue to function correctly\n   - Verify that the changes don't negatively impact related features\n\nDocument all test results and any issues encountered during testing.",
      "subtasks": [
        {
          "id": "21.1",
          "title": "Create migration script 007_update_planet_history.sql",
          "description": "Create a migration script to update the planet_history table by adding current_health (BIGINT), max_health (BIGINT), and player_count (INT) columns to match the actual API response structure.",
          "status": "done"
        },
        {
          "id": "21.2",
          "title": "Run migration to update schema",
          "description": "Execute the prepared migration script on both live and test databases to update the planet_history table schema with the new columns (current_health, max_health, player_count).",
          "status": "done"
        },
        {
          "id": "21.3",
          "title": "Update data import scripts",
          "description": "Modify the import scripts to correctly parse and store the additional fields from the API response (current_health, max_health, player_count).",
          "status": "done"
        },
        {
          "id": "21.4",
          "title": "Update service and repository layers",
          "description": "Update the service and repository layers to handle the new fields in the planet_history table.",
          "status": "done"
        },
        {
          "id": "21.5",
          "title": "Update documentation",
          "description": "Update API documentation and developer guides to reflect the changes to the planet_history table schema.",
          "status": "done"
        },
        {
          "id": "21.6",
          "title": "Implement and run tests",
          "description": "Create and execute tests to verify the updated schema and import process work correctly with the actual API response structure.",
          "status": "done"
        }
      ]
    },
    {
      "id": 22,
      "title": "Task #22: Rebuild War Info API Integration to Match Actual API Response Structure",
      "description": "Update the database schema, import scripts, and related components to correctly handle the actual structure of the war info API response, ensuring all fields are properly stored and retrieved.",
      "details": "This task involves a comprehensive rebuild of the War Info API integration to align with the actual API response structure. The implementation should follow these steps:\n\n1. Analyze the actual War Info API response structure to identify all fields, data types, and relationships.\n2. Update the database schema to accommodate the actual structure:\n   - Create a migration script using the drop-and-rebuild approach (reference Task #16)\n   - Ensure proper field types and constraints are defined\n   - Add appropriate indexes for performance optimization\n\n3. Update the data transformer:\n   - Identify and convert integer timestamp fields to MySQL DATETIME strings\n   - Handle any nested objects or arrays appropriately\n   - Implement proper error handling for missing or malformed fields\n\n4. Modify the import scripts to:\n   - Parse the API response correctly\n   - Transform data according to the updated schema\n   - Validate data before insertion\n   - Log any errors or inconsistencies\n\n5. Update related components:\n   - Adjust any services or controllers that interact with war info data\n   - Update data access objects or repositories\n   - Modify any caching mechanisms if applicable\n\n6. Update API documentation to reflect the new structure and field definitions.\n\n7. Follow the established project patterns for error handling, logging, and performance considerations.\n\nNote: Pay special attention to integer timestamp fields in the API response, as these need to be properly converted to MySQL DATETIME format in the transformer.",
      "testStrategy": "To verify the successful completion of this task, implement the following testing strategy:\n\n1. Unit Tests:\n   - Test the data transformer to ensure proper conversion of integer timestamps to DATETIME strings\n   - Verify correct handling of all fields in the API response\n   - Test edge cases such as missing fields, null values, and unexpected data types\n\n2. Integration Tests:\n   - Create tests that simulate API responses with sample data\n   - Verify the entire pipeline from API response to database storage\n   - Test the retrieval of stored data to ensure it matches the original API response\n\n3. Database Verification:\n   - Execute the migration script in a test environment\n   - Verify the schema matches the expected structure\n   - Check constraints, indexes, and relationships\n\n4. Manual Testing:\n   - Perform a complete end-to-end test with real API data\n   - Verify all fields are correctly stored and retrieved\n   - Compare the stored data with the original API response\n\n5. Documentation Review:\n   - Ensure API documentation is updated to reflect the new structure\n   - Verify code comments are clear and accurate\n\n6. Performance Testing:\n   - Measure query performance for common operations\n   - Verify that large datasets can be processed efficiently\n\n7. Regression Testing:\n   - Ensure that other components dependent on war info data continue to function correctly\n   - Verify that the changes don't negatively impact existing functionality",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 23,
      "title": "Add error reporting for missing Planet IDs during planet history ingestion and tests",
      "description": "Implement logic to collect and report all missing planet_id errors (foreign key violations) encountered during planet history ingestion or testing. The report should include the missing planet IDs and relevant context for each occurrence.",
      "status": "done",
      "dependencies": [],
      "priority": "medium",
      "details": "The implementation approach will involve:\n\n1. Capturing foreign key violations during planet history ingestion\n2. Collecting all missing planet_id errors in a structured format\n3. Including contextual information such as timestamp, operation type, and data source\n4. Generating a comprehensive error report that can be reviewed by administrators\n5. Ensuring the system continues processing valid records while logging invalid ones",
      "testStrategy": "Testing will include:\n\n1. Unit tests with mock data containing known missing planet IDs\n2. Integration tests verifying error collection during actual ingestion processes\n3. Validation that all error contexts are properly captured\n4. Verification that the error report format is clear and actionable",
      "subtasks": []
    },
    {
      "id": 24,
      "title": "Task #24: Refactor Testing Framework to Use Pytest",
      "description": "Standardize the project's testing approach by migrating from individual test scripts to pytest as the primary test runner, including proper implementation of fixtures, test discovery, and reporting mechanisms.",
      "status": "done",
      "dependencies": [],
      "priority": "medium",
      "details": "This task involves a comprehensive refactoring of the existing test suite to leverage pytest's capabilities:\n\n1. **Migration to pytest structure**:\n   - Convert all existing test files to follow pytest naming conventions (test_*.py or *_test.py)\n   - Restructure test directories to enable automatic test discovery\n   - Remove any manual test runners or main() functions from test files\n   - Update import statements to use relative imports where appropriate\n\n2. **Implement pytest fixtures**:\n   - Create fixtures for common setup/teardown operations\n   - Develop a database fixture that properly connects to helldivers2_test database\n   - Ensure database state is properly reset between tests\n   - Implement session-level fixtures for expensive operations that can be reused\n\n3. **Database connection management**:\n   - Create a centralized configuration for test database credentials\n   - Implement environment variable handling for database connection parameters\n   - Ensure tests use isolated database connections to prevent cross-test contamination\n   - Add transaction management to roll back changes after tests\n\n4. **Test isolation and repeatability**:\n   - Ensure each test can run independently\n   - Implement proper cleanup mechanisms\n   - Address any tests with external dependencies\n   - Eliminate any test order dependencies\n\n5. **Documentation updates**:\n   - Update README with detailed instructions on running tests with pytest\n   - Document any custom fixtures and their usage\n   - Include examples of running specific test subsets\n   - Add information about test reporting options\n\n6. **CI/CD integration**:\n   - Update any CI/CD pipelines to use pytest\n   - Configure pytest to generate reports compatible with CI tools\n   - Set up proper exit codes for test failures",
      "testStrategy": "The refactoring will be verified through the following steps:\n\n1. **Functionality verification**:\n   - Run the complete test suite with pytest to ensure all tests pass\n   - Compare test results before and after refactoring to confirm no functionality was lost\n   - Verify that all tests can be discovered automatically by pytest\n\n2. **Isolation testing**:\n   - Run individual tests in isolation to verify they don't depend on other tests\n   - Run tests in different orders to ensure they don't rely on execution sequence\n   - Verify tests can be run in parallel without conflicts\n\n3. **Database connection validation**:\n   - Confirm tests properly connect to helldivers2_test database\n   - Verify database state is properly reset between test runs\n   - Test with different environment configurations to ensure connection parameters work correctly\n   - Check that no test data leaks between test runs\n\n4. **Documentation review**:\n   - Verify README accurately describes how to run tests with pytest\n   - Ensure all custom fixtures are properly documented\n   - Confirm the documentation includes examples for common test scenarios\n\n5. **CI/CD verification**:\n   - Run the test suite in the CI/CD pipeline to confirm integration\n   - Verify test reports are generated correctly\n   - Ensure the pipeline fails appropriately when tests fail\n\n6. **Code review**:\n   - Conduct a thorough review of the refactored test code\n   - Verify all main() functions and manual test runners have been removed\n   - Confirm proper use of pytest fixtures and assertions",
      "subtasks": [
        {
          "id": 1,
          "title": "Deduplicate test DB setup fixtures using conftest.py",
          "description": "Move the shared setup_test_db_env and clean_db_before_and_after fixtures to a single conftest.py file in src/. Remove duplicate fixture definitions from all test files and ensure all tests use the shared fixtures for DB setup and teardown. This will enforce DRY principles and simplify test maintenance.",
          "details": "- Create src/conftest.py with the shared fixtures for DB environment and cleanup.\n- Remove the same fixtures from all test_*.py files in src/.\n- Confirm all tests still run and use the correct DB credentials and cleanup logic.",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 24
        },
        {
          "id": 2,
          "title": "Add pytest markers for fast vs complete test runs",
          "description": "Add pytest markers to distinguish between fast (unit) tests and complete (including integration/orchestration) tests. Update pytest.ini and test files accordingly. Document usage in README if needed.",
          "details": "- Mark the orchestration/integration test(s) with @pytest.mark.complete\n- All other tests are considered 'fast' by default\n- Add marker registration to pytest.ini\n- Document how to run only fast tests (pytest -m fast), only complete tests (pytest -m complete), or all tests (pytest)\n- Ensure the orchestration test is excluded from 'fast' runs\n- Update README with new test running instructions if necessary.",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 24
        },
        {
          "id": 3,
          "title": "Complete migration to pytest structure",
          "description": "Finalize the migration of all test files to follow pytest conventions and ensure automatic test discovery works correctly.",
          "details": "- Verify all test files follow pytest naming conventions\n- Confirm test directories are properly structured\n- Remove any remaining manual test runners\n- Update import statements as needed\n- Ensure all tests are discoverable by pytest",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 24
        },
        {
          "id": 4,
          "title": "Finalize database connection management",
          "description": "Complete the implementation of centralized database configuration and connection management for tests.",
          "details": "- Finalize centralized configuration for test database credentials\n- Verify environment variable handling for connection parameters\n- Ensure test isolation through proper connection management\n- Implement transaction rollback for test cleanup",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 24
        },
        {
          "id": 5,
          "title": "Update documentation and CI/CD integration",
          "description": "Complete documentation updates and integrate pytest with CI/CD pipelines.",
          "details": "- Finalize README updates with comprehensive test instructions\n- Document all custom fixtures and their usage\n- Update CI/CD pipelines to use pytest\n- Configure test reporting for CI tools\n- Verify proper exit code handling for test failures",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 24
        },
        {
          "id": 6,
          "title": "Final verification and review",
          "description": "Perform final verification of the refactored test framework and conduct a comprehensive review.",
          "details": "- Run complete test suite to verify all tests pass\n- Confirm test isolation and repeatability\n- Verify database state management between tests\n- Review all code changes for quality and consistency\n- Ensure all requirements of the task have been met",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 24
        }
      ]
    },
    {
      "id": 25,
      "title": "Task #25: Implement Chaos Bot for Helldivers 2 API-to-DB Pipeline Testing",
      "description": "Create a configurable Chaos Bot that simulates and injects various production issues into the Helldivers 2 API-to-DB pipeline to test its robustness, error handling, and recovery capabilities.",
      "details": "The Chaos Bot should be implemented as a separate module that can intercept and modify API responses or database operations in the pipeline. Key implementation details include:\n\n1. Design fault injection scenarios:\n   - API timeouts and connection failures\n   - Malformed JSON responses (invalid structure, missing fields, unexpected types)\n   - Partial data responses\n   - Rate limiting simulation\n   - DB connection drops and timeouts\n   - Foreign key violations (missing references)\n   - Duplicate data entries\n   - Schema validation failures\n   - Intermittent failures (random timing)\n\n2. Implementation requirements:\n   - Create a modular architecture allowing easy addition of new fault types\n   - Implement configuration system via YAML/JSON files to control:\n     - Types of faults to inject\n     - Frequency/probability of each fault\n     - Timing parameters (duration, intervals)\n     - Severity levels\n   - Develop hooks/interception points in the pipeline for fault injection\n   - Implement comprehensive logging of all injected faults with timestamps\n   - Create a summary report generator showing:\n     - Total faults injected by type\n     - Pipeline response to each fault (recovery, failure, etc.)\n     - Performance impact metrics\n     - Recommendations for improvements\n\n3. Integration with existing pipeline:\n   - Modify the pipeline to allow optional Chaos Bot integration\n   - Ensure the bot can be enabled/disabled via configuration\n   - Add command-line flags for chaos testing mode\n   - Implement safeguards to prevent accidental use in production\n\n4. Documentation:\n   - Create detailed usage documentation\n   - Document all available fault types and configuration options\n   - Provide examples of common testing scenarios\n   - Include troubleshooting guide for the Chaos Bot itself",
      "testStrategy": "Testing the Chaos Bot implementation should follow these steps:\n\n1. Unit Testing:\n   - Test each fault injection mechanism in isolation\n   - Verify configuration parsing and validation\n   - Test logging functionality and report generation\n   - Ensure proper cleanup after fault injection\n\n2. Integration Testing:\n   - Test the Chaos Bot with a staging/test version of the pipeline\n   - Verify that each fault type can be properly injected\n   - Confirm that the pipeline's error handling responds appropriately\n   - Test that the bot correctly logs all activities\n\n3. Validation Testing:\n   - Create specific test scenarios that combine multiple fault types\n   - Verify that the summary reports accurately reflect injected faults\n   - Test edge cases (extremely high fault rates, long-duration faults)\n   - Ensure the bot can be safely disabled/enabled without affecting normal operation\n\n4. Performance Testing:\n   - Measure the overhead introduced by the Chaos Bot when enabled\n   - Verify that the bot itself doesn't become a bottleneck\n   - Test with various configuration settings to ensure scalability\n\n5. Acceptance Criteria:\n   - All planned fault types can be successfully injected\n   - The bot produces accurate logs and summary reports\n   - The pipeline's error handling can be effectively evaluated\n   - Configuration changes take effect without requiring restarts\n   - Documentation is comprehensive and includes examples\n   - The bot can be completely disabled with no residual effects on the pipeline\n\n6. Manual Testing:\n   - Conduct a \"chaos engineering session\" where team members use the bot to identify potential weaknesses in the pipeline\n   - Document any unexpected behaviors or failures discovered",
      "status": "pending",
      "dependencies": [],
      "priority": "medium",
      "subtasks": [
        {
          "id": 1,
          "title": "Design and implement core Chaos Bot architecture",
          "description": "Create the foundational architecture for the Chaos Bot, including the module structure, configuration system, and basic interception framework.",
          "dependencies": [],
          "details": "Implement a modular architecture with: 1) A core Bot class that manages fault injection, 2) A configuration parser for YAML/JSON files, 3) An interception framework with hooks for API and DB operations, 4) A fault registry system for registering different fault types, and 5) Basic command-line interface for enabling/disabling the bot. The configuration system should support specifying fault types, frequencies, timing parameters, and severity levels.",
          "status": "pending",
          "testStrategy": "Unit test each component independently. Create mock pipeline components to verify interception points work correctly. Test configuration parsing with various valid and invalid inputs."
        },
        {
          "id": 2,
          "title": "Implement fault injection scenarios",
          "description": "Develop the specific fault injection modules for all required scenarios, including API faults, data corruption, and database issues.",
          "dependencies": [
            1
          ],
          "details": "Create separate fault injector classes for each fault category: 1) NetworkFaultInjector for API timeouts and connection failures, 2) DataFaultInjector for malformed JSON and partial responses, 3) RateLimitInjector for simulating API rate limits, 4) DatabaseFaultInjector for DB connection issues and query failures, 5) ValidationFaultInjector for schema and foreign key violations. Each injector should implement a common interface defined in the core architecture and include configuration for probability, duration, and severity.",
          "status": "pending",
          "testStrategy": "Test each fault injector with unit tests. Create integration tests that verify each fault type can be properly injected and detected in a controlled environment."
        },
        {
          "id": 3,
          "title": "Integrate Chaos Bot with the Helldivers 2 API-to-DB pipeline",
          "description": "Modify the existing pipeline to support Chaos Bot integration, adding interception points and safety mechanisms.",
          "dependencies": [
            2
          ],
          "details": "1) Identify and implement interception points in the API client, data transformation layer, and database access components, 2) Add configuration flags to enable/disable Chaos Bot at runtime, 3) Implement environment detection to prevent accidental use in production, 4) Create a pipeline wrapper that can route operations through the Chaos Bot when enabled, 5) Add graceful degradation mechanisms to ensure the pipeline can recover from injected faults when possible.",
          "status": "pending",
          "testStrategy": "Create integration tests that run the full pipeline with Chaos Bot enabled. Verify that each interception point correctly triggers fault injection. Test environment detection and safety mechanisms."
        },
        {
          "id": 4,
          "title": "Implement logging and reporting system",
          "description": "Develop comprehensive logging of injected faults and a reporting system to analyze pipeline resilience.",
          "dependencies": [
            3
          ],
          "details": "1) Implement detailed logging of all fault injections including timestamp, fault type, severity, and duration, 2) Create a metrics collection system to track pipeline response times and recovery rates, 3) Develop a summary report generator that shows statistics on injected faults, pipeline responses, and performance impacts, 4) Add visualization capabilities for fault frequency and impact, 5) Implement recommendations engine that suggests pipeline improvements based on observed failures.",
          "status": "pending",
          "testStrategy": "Verify logs contain all required information. Test report generation with various fault injection scenarios. Validate metrics collection accuracy by comparing with manual calculations."
        },
        {
          "id": 5,
          "title": "Create documentation and usage examples",
          "description": "Develop comprehensive documentation for the Chaos Bot, including usage guides, configuration options, and troubleshooting information.",
          "dependencies": [
            4
          ],
          "details": "1) Create detailed API documentation for all Chaos Bot components, 2) Write a user guide explaining configuration options and command-line flags, 3) Develop examples of common testing scenarios with sample configuration files, 4) Create a troubleshooting guide for common issues, 5) Document all available fault types with their parameters and expected impacts, 6) Include a section on interpreting reports and implementing recommended improvements.",
          "status": "pending",
          "testStrategy": "Review documentation for completeness and accuracy. Have team members attempt to use the Chaos Bot following only the documentation to identify gaps or unclear instructions."
        }
      ]
    },
    {
      "id": 26,
      "title": "Task #26: Implement Dual Database Credential Support with Test Environment Isolation",
      "description": "Add support for separate live and test database credentials in the .env file and modify the test framework to automatically use test database settings during test execution, ensuring test isolation from production data.",
      "details": "This task involves several key components:\n\n1. **Environment File Structure**:\n   - Update the .env file format to include two sets of database credentials:\n     - LIVE_DB_HOST, LIVE_DB_PORT, LIVE_DB_NAME, LIVE_DB_USER, LIVE_DB_PASSWORD\n     - TEST_DB_HOST, TEST_DB_PORT, TEST_DB_NAME, TEST_DB_USER, TEST_DB_PASSWORD\n   - Ensure backward compatibility by maintaining support for the existing DB_* variables, which should map to the live database settings\n\n2. **Configuration Logic**:\n   - Modify the database connection module to read both sets of credentials\n   - Implement a connection factory that can create connections to either database based on a runtime parameter\n   - Add an environment variable (e.g., USE_TEST_DB=true/false) to control which database is used\n\n3. **Test Framework Updates**:\n   - Update conftest.py to use pytest's monkeypatch.setenv to override the database environment variables during test execution\n   - Create a fixture that ensures all tests use the test database credentials\n   - Implement the fixture to run automatically for all tests without explicit inclusion\n\n4. **Documentation**:\n   - Update the README.md with clear instructions on:\n     - The new .env file format with examples\n     - How the system determines which database to use\n     - How to manually switch between databases for development\n     - The automatic test isolation mechanism\n\n5. **Safety Mechanisms**:\n   - Add validation to prevent accidental use of live database in test environments\n   - Implement clear logging when database connections are established to indicate which database is being used",
      "testStrategy": "The implementation should be verified through the following tests:\n\n1. **Environment Variable Parsing Tests**:\n   - Verify that both sets of credentials are correctly parsed from the .env file\n   - Test fallback behavior when only legacy variables are present\n   - Test error handling for missing or incomplete credentials\n\n2. **Connection Factory Tests**:\n   - Create unit tests that verify the connection factory correctly uses the appropriate credentials based on the environment setting\n   - Test that connection strings are properly constructed with the correct parameters\n\n3. **Test Isolation Verification**:\n   - Create a test that attempts to write to a test-only table\n   - Verify that this operation succeeds in the test environment\n   - Verify that the same code running outside of tests (without monkeypatch) would use the live database\n\n4. **Integration Tests**:\n   - Run the full test suite with deliberate differences between test and live database configurations\n   - Verify through logs or connection monitoring that all test connections use the test database\n   - Create a canary test that fails if it detects it's running against the live database\n\n5. **Documentation Verification**:\n   - Review the README updates for clarity and completeness\n   - Have another team member follow the documentation to set up a development environment with both database configurations\n\n6. **Edge Case Testing**:\n   - Test behavior when credentials are invalid\n   - Test behavior when switching between databases at runtime\n   - Verify that long-running processes maintain the correct database connection",
      "status": "done",
      "dependencies": [],
      "priority": "medium",
      "subtasks": [
        {
          "id": 1,
          "title": "Update .env file structure and environment variable handling",
          "description": "Modify the .env file format to support dual database credentials while maintaining backward compatibility with existing DB_* variables.",
          "dependencies": [],
          "details": "1. Create a new .env.example file that includes both LIVE_DB_* and TEST_DB_* variable sets\n2. Update the environment variable loading module to read both sets of credentials\n3. Implement fallback logic where if LIVE_DB_* variables aren't defined, the system uses DB_* variables\n4. Add the USE_TEST_DB environment variable with boolean parsing\n5. Document the changes in code comments",
          "status": "done",
          "testStrategy": "Write unit tests for the environment variable loading module to verify it correctly handles various .env configurations including missing variables and fallback scenarios."
        },
        {
          "id": 2,
          "title": "Implement database connection factory with credential switching",
          "description": "Create a connection factory that can dynamically select between live and test database credentials based on runtime parameters.",
          "dependencies": [
            1
          ],
          "details": "1. Refactor the existing database connection module to use a factory pattern\n2. Implement a getDatabaseConnection() function that accepts an optional useTestDb parameter\n3. Use the USE_TEST_DB environment variable as the default value if the parameter isn't provided\n4. Add clear logging when connections are established to indicate which database is being used\n5. Implement validation to prevent accidental use of live database in test environments (check NODE_ENV)",
          "status": "done",
          "testStrategy": "Create unit tests that verify the connection factory correctly selects credentials based on the useTestDb parameter and environment variables."
        },
        {
          "id": 3,
          "title": "Update test framework with automatic test database selection",
          "description": "Modify the test framework to automatically use test database credentials during test execution.",
          "dependencies": [
            2
          ],
          "details": "1. Update conftest.py to use pytest's monkeypatch.setenv to override USE_TEST_DB to 'true'\n2. Create a database fixture that ensures all tests use the test database credentials\n3. Configure the fixture to run automatically for all tests without explicit inclusion\n4. Add teardown logic to close database connections after tests complete\n5. Implement safeguards to verify tests are never running against the live database",
          "status": "done",
          "testStrategy": "Create a meta-test that verifies the test framework is correctly using the test database by checking connection parameters during test execution."
        },
        {
          "id": 4,
          "title": "Implement test isolation verification mechanism",
          "description": "Create utilities to verify that tests are properly isolated from the production database.",
          "dependencies": [
            3
          ],
          "details": "1. Create a TestDatabaseVerifier class that can be used in tests\n2. Implement methods to check which database is currently being used\n3. Add a function to verify test data isolation by checking database connection parameters\n4. Create helper methods for tests to assert they're running against the test database\n5. Add logging during test execution to show which database is being used",
          "status": "done",
          "testStrategy": "Write tests that use the verification mechanism to confirm they're running against the test database and that isolation is maintained throughout the test suite."
        },
        {
          "id": 5,
          "title": "Update documentation and create migration guide",
          "description": "Update the README.md and create additional documentation to explain the new dual database credential system.",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "1. Update the README.md with clear instructions on the new .env file format with examples\n2. Document how the system determines which database to use\n3. Provide instructions on how to manually switch between databases for development\n4. Explain the automatic test isolation mechanism\n5. Create a migration guide for existing projects to adopt the new dual credential system",
          "status": "done",
          "testStrategy": "Have team members review the documentation and attempt to follow the instructions to verify clarity and completeness."
        }
      ]
    }
  ]
}